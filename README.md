# GUIMC Deep Learning
Адаптированный курс для ГУИМЦ по глубокому обучению

### Термины и определения

1. Искусственная нейронная сеть - математическая модель, состоящая из слоев нейронных, соединенных связями 
2. Нейрон - элемент нейронной сети, который позволяет получить новое значение на основании его входов за счет математических операций
3. Набор данных - совокупность данных, состоящая из множества экземпляров, необходимых для обучения и верификации (тестирования) нейронной сети
4. Стохастический градиентный спуск - итерационный метод обучения нейронной сети, который заключается в минимизации функции потерь с помощью вычисления градиента
5. Функция потерь - функция, которая позволяет получить значение совокупной ошибки нейронной сети для каждого набора значений ее параметров 
6. Метод обратного распространения ошибки - метод нахождения приращения для каждого веса нейронной сети на основе градиента ее функции потерь (ошибки) и производной сложной функции
7. Переобучение - этап в обучении нейронной сети, когда начинает возрастать ошибка на тестовой выборке при уменьшении ошибки на обучающей выборке из-за ее чрезмерной сложности модели
8. Регуляризация - совокупность методов для борьбы с переобучением и усложнением модели
9. Дропаут - вид регуляризации, который заключается в случайном исключении весов нейронной сети из процесса обучения 
10. Аугментация данных - расширение выборки (набора данных) за счет различных преобразований
11. Перенос обучения - процесс использования значений весов предварительно обученной нейронной сети для обучения новой на другом наборе данных
12. Заморозка весов - процедура исключения части параметров нейронной сети из процесса обучения для фиксации их значений
13. Веб-приложение - клиент-серверное приложение, которое использует протокол HTTP для взаимодействия 
14. PyTorch - фреймворк для обучения нейронных сетей
15. ONNX - библиотека, позволяющая конвертировать обученные модели нейронных сетей в различные форматы для использования с разными языками программирования
16. Гиперпараметры - скорость обучения, размер батча и другие переменные, которые фиксируются и не меняются в процессе обучения в отличие от параметров модели (весов нейронной сети)


### Лекция 1. Введение в обучение с учителем: 

[Презентации 1-5 лекции](https://github.com/iu5git/GUIMC-Deep-Learning/blob/main/lections/lection_1.pdf)

- Введение в машинное обучение: определение; общий алгоритм обучения нейронных сетей.  
- Задачи, в которых применяются нейросети
- Типы машинного обучения: типы обучения; обучение с учителем; обучение с подкреплением

### Лекция 2. Полносвязная нейронная сеть: 
- Нейронная сеть: схема и определения; пример полносвязной нейронной сети
- Пример PyTorch: активационная функция; пример PyTorch
- Вычисления в нейронной сети при прямом проходе

### Лекция 3. Набор данных
- Тестовая и обучающая выборки: набор данных; тестовая и обучающая выборки
- Набор данных Cifar100
- Метрики оценки качества

### Лекция 4. Метод градиентного спуска
- Функция потерь: функция потерь; метод наименьших квадратов; перекрестная энтропия
- Градиентный спуск: градиентный спуск; шаги обучения
- Обратное распространение ошибки

### Лекция 5. Гиперпараметры обучения
- Эпохи обучения
- Размер батча и итерации: итерации и эпохи; пакетный спуск
- Скорость обучения: скорость обучения; оптимальные значения

### Лекция 6. Разбор практического примера на PyTorch

[Ссылка на задание](/notebooks/Lab1.ipynb)
- Загрузка набора данных
- Подготовка модели: описание модели; оптимизатор и функция потерь
- Результаты обучения: результаты по эпохам, точность и другие метрики

### Лекция 7. Сверточные нейронные сети

[Презентации 7-8 лекции](https://github.com/iu5git/GUIMC-Deep-Learning/blob/main/lections/lection_2.pdf)
- Cifar, три цвета и каналы
- Свертка: свойства свертки; пример двумерной свертки
- Сверточная нейронная сеть: общая схема по слоям; пример вычисления

### Лекция 8. Паддинг, пуллинг, страйд
- Пуллинг
- Паддинг
- Страйд - шаг свертки 

### Лекция 9. Практика 2. Сверточная нейронная сеть и сравнение

[Ссылка на задание](/notebooks/Lab2.ipynb)
- создание сверточной нейронной сети
- Сохранение в формате ONNX
- Оценка качества нейронной сети в html

### Лекция 10. Различные виды оптимизации

[Презентация 10 лекции](https://github.com/iu5git/GUIMC-Deep-Learning/blob/main/lections/lection_3.pdf)
- Алгоритм стохастического спуска
- Первый момент: спуск с моментом; adagrad
- Второй момент: Rmsprop; Adam 

### Лекция 11. Регуляризация

[Презентации 11-12 лекции](https://github.com/iu5git/GUIMC-Deep-Learning/blob/main/lections/lection_4.pdf)
- Определения: переобучение; регуляризация
- Дропаут 
- Другие методы: штраф за сложность; сглаживание меток

### Лекция 12. Аугментация данных
- Что такое аугментация данных
- Афинные преобразования: сдвиг; поворот и отражение 
- Изменения цвета

### Лекция 13. Практика 3. Реализация аугментации и регуляризации в PyTorch

[Ссылка на задание](/notebooks/Lab3.ipynb)
- Пример аугментации данных
- Пример добавления dropout
- Пример штрафа и сглаживания

### Лекция 14. Трансферное обучение модели на Cifar

[Презентация 14 лекции](https://github.com/iu5git/GUIMC-Deep-Learning/blob/main/lections/lection_5.pdf)
- Предобученные модели
- Перенос обучения
- Дообучение

### Лекция 15. Практика по переносу обучения

[Ссылка на задание](/notebooks/Lab4.ipynb)
- Модель: загрузка предобученной модели; добавление полносвязных слоев
- Обучение: заморозка весов; обучение полносвязных слоев
- Дообучение

### Лекция 16. Практика по сбору собственного набора данных

[Ссылка для 16-17 занятия](/homework)
- Сбор данных в интернет
- Разметка данных
- Загрузка набора в ноутбук

### Лекция 17. Практика по созданию приложения на Django
- Создание проекта Django
- Добавление функции для предсказания 
- Использование приложения
